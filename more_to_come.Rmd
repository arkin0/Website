---
title: "More to Come"
output:
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE, cache = TRUE)
```
```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```


# Reinforcement Learning

## Upper confindence Bound (UCB)

### Importing the dataset

```{r}
dataset = read.csv('Ads_CTR_Optimisation.csv')
```

### Implementing UCB

```{r}
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_selections = integer(d)
sums_of_rewards = integer(d)
total_reward = 0
for (n in 1:N) {
  ad = 0
  max_upper_bound = 0
  for (i in 1:d) {
    if (numbers_of_selections[i] > 0) {
      average_reward = sums_of_rewards[i] / numbers_of_selections[i]
      delta_i = sqrt(3/2 * log(n) / numbers_of_selections[i])
      upper_bound = average_reward + delta_i
    } else {
        upper_bound = 1e400
    }
    if (upper_bound > max_upper_bound) {
      max_upper_bound = upper_bound
      ad = i
    }
  }
  ads_selected = append(ads_selected, ad)
  numbers_of_selections[ad] = numbers_of_selections[ad] + 1
  reward = dataset[n, ad]
  sums_of_rewards[ad] = sums_of_rewards[ad] + reward
  total_reward = total_reward + reward
}

```


#### Visualising the results

```{r}
hist(ads_selected,
     col = 'blue',
     main = 'Histogram of ads selections',
     xlab = 'Ads',
     ylab = 'Number of times each ad was selected')
```

   

## Thompson Sampling


### Importing the dataset

```{r}
dataset = read.csv('Ads_CTR_Optimisation.csv')
```

### Implementing Thompson Sampling
```{r}
N = 10000
d = 10
ads_selected = integer(0)
numbers_of_rewards_1 = integer(d)
numbers_of_rewards_0 = integer(d)
total_reward = 0
for (n in 1:N) {
  ad = 0
  max_random = 0
  for (i in 1:d) {
    random_beta = rbeta(n = 1,
                        shape1 = numbers_of_rewards_1[i] + 1,
                        shape2 = numbers_of_rewards_0[i] + 1)
    if (random_beta > max_random) {
      max_random = random_beta
      ad = i
    }
  }
  ads_selected = append(ads_selected, ad)
  reward = dataset[n, ad]
  if (reward == 1) {
    numbers_of_rewards_1[ad] = numbers_of_rewards_1[ad] + 1
  } else {
    numbers_of_rewards_0[ad] = numbers_of_rewards_0[ad] + 1
  }
  total_reward = total_reward + reward
}
```

### Visualising the results
```{r}
hist(ads_selected,
     col = 'blue',
     main = 'Histogram of ads selections',
     xlab = 'Ads',
     ylab = 'Number of times each ad was selected')

```

# Natural Language Processing


## Importing the dataset
```{r}
dataset_original = read.delim('Restaurant_Reviews.tsv', quote = '', stringsAsFactors = FALSE)
```

## Cleaning the texts

```{r}
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
corpus = tm_map(corpus, content_transformer(tolower))
corpus = tm_map(corpus, removeNumbers)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords())
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, stripWhitespace)
```

## Creating the Bag of Words model

```{r}
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
```

## Importing the dataset
```{r}
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
```
## Encoding the target feature as factor
```{r}
dataset$Liked = factor(dataset$Liked, levels = c(0, 1))
```

## Splitting the dataset into the Training set and Test set

```{r}
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

## Fitting Random Forest Classification to the Training set

```{r}
library(randomForest)
classifier = randomForest(x = training_set[-692],
                          y = training_set$Liked,
                          ntree = 10)
```

## Predicting the Test set results
```{r}
y_pred = predict(classifier, newdata = test_set[-692])
```

## Making the Confusion Matrix
```{r}
cm = table(test_set[, 692], y_pred)
```

# Deep Learning

## Artificial Neural Networks (ANN)

### Importing the dataset
```{r}
dataset = read.csv('Churn_Modelling.csv')
dataset = dataset[4:14]
```

### Encoding the categorical variables as factors

```{r}
dataset$Geography = as.numeric(factor(dataset$Geography,
                                      levels = c('France', 'Spain', 'Germany'),
                                      labels = c(1, 2, 3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
                                   levels = c('Female', 'Male'),
                                   labels = c(1, 2)))
```

### Splitting the dataset into the Training set and Test set

```{r}
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
```

### Feature Scaling
```{r}
training_set[-11] = scale(training_set[-11])
test_set[-11] = scale(test_set[-11])
```

### Fitting ANN to the Training set

```{r}
library(h2o)
h2o.init(nthreads = -1)
model = h2o.deeplearning(y = 'Exited',
                         training_frame = as.h2o(training_set),
                         activation = 'Rectifier',
                         hidden = c(5,5),
                         epochs = 100,
                         train_samples_per_iteration = -2)
```

### Predicting the Test set results
```{r}
y_pred = h2o.predict(model, newdata = as.h2o(test_set[-11]))
y_pred = (y_pred > 0.5)
y_pred = as.vector(y_pred)
```
###Making the Confusion Matrix
```{r}
cm = table(test_set[, 11], y_pred)
h2o.shutdown()
```

## Convolutional Neural Networks (CNN)


Pas de code sous R.... 

# Instrumental Variables Regression 

Simple linear regression model (OLS) is based on the assumption that the independent variables are exogenous. That is, the error terms in the linear regression model are uncorrelated or independent of the explanatory
variables.


Explanatory variables that are not exogenous are called
endogenous variables. Therefore, if, for whatever reason, an explanatory variable is correlated with the model error term then it is said to be an endogenous explanatory variable and we say the model suffers from endogeneity. 

All OLS estimators will be biased and inconsistent in the presence of endogenous regressors. Endogeneity can arise as a result of measurement error, reverse casualty/simultaneity, omitted variable or unobserved variables, omitted selection,lagged dependent variables. 

To solve endogeneity, we should eliminate measurement error, introduce omitted or unobserved variables for mending the correlated missing regressors, narrow the generality of interpretation for mending sample selection bias as it does no
longer apply to the population, but rather to the chunk of the population satisfying sample restrictions, estimate a system
of simultaneous equations etc. Often these solutions are not achievable in practice. Reverse Causality is also tougher to
handle. Thus the solution is to use an alternative estimation method known as instrumental variables (IV) or equivalently
two-stage least squares (2SLS). This involves introduction of a variable that induces changes in the explanatory variable
but has no independent effect on the dependent variable, allowing one to uncover the causal effect of the
explanatory variable on the dependent variable. 

We try to estimating the return to education for Married Women. T.A. Mroz (1987), The Sensitivity of an Empirical Model of Married Women’s Hours of Work to Economic and Statistical Assumptions, Econometrica 55, 765-799. Professor Ernst R. Berndt, of MIT, kindly provided the data, which he obtained from Professor Mroz.


```{r echo=TRUE, message=FALSE}
library(wooldridge)
library(stargazer)
data("mroz")
head(mroz)
```

### We fit the basic model
```{r, echo=TRUE}
educ_model <- lm(lwage ~ educ, data = mroz)
```

We run the typical linear model, but notice the use of the subset argument. inlf is a binary variable in which a value of 1 means they are “In the Labor Force”. By sub-setting the mroz data.frame by observations in which inlf==1, only working women will be in the sample.


```{r, echo=TRUE}
fatheduc_model <- lm(educ ~ fatheduc, data = mroz, subset = (inlf==1))
```

### Instrumental Variable Regression
```{r, echo=TRUE}
library("AER")
educ_IV_model <- ivreg(lwage ~ educ | fatheduc, data = mroz)
```

We use coeftest() in conjunction with vcovHC() to obtain robust coefficient summaries for all models.

```{r, echo=TRUE}
coeftest(educ_IV_model, vcov = vcovHC, type = "HC1")
```

### Check Instrument Validity

Instruments that explain little variation in the endogenous regressor
are called weak instruments. Weak instruments provide little information about the variation in  explicative variable that is exploited by IV regression to estimate the effect of interest: the estimate of the coefficient on the endogenous regressor is estimated inaccurately. Moreover, weak instruments cause the distribution of the estimator to deviate considerably from a normal distribution even in large samples such that the usual methods for obtaining inference about the true coefficient on  explicative variable may produce wrong results.

We proceed by generating a tabulated summary of the estimation results. First we gather standard errors in a list : 

```{r, echo=TRUE}
rob_se <- list(sqrt(diag(vcovHC(educ_model, type = "HC1"))),
               sqrt(diag(vcovHC(educ_IV_model, type = "HC1"))))
```

Then we generate table with stargazer() :
```{r, echo=TRUE}
library(stargazer)
stargazer(educ_model, educ_IV_model,
  type = "text",
  header = FALSE, 
  omit.table.layout = "n",
  digits = 2, 
  column.labels = c("Original model", "IV: fathereduc"),
  dep.var.labels.include = FALSE,
  dep.var.caption = "Dependent Variable: Log salary",
  se = rob_se)
```

Which one should we trust? This hinges on the validity of the instruments used. To assess this we compute  
F -statistics for the first-stage regressions of our model to check instrument relevance.

```{r, echo=TRUE}
mod_relevance1 <- lm(lwage ~ educ + fatheduc, data=mroz)
```

and we check the linearHypothesis
```{r, echo=TRUE}
linearHypothesis(mod_relevance1, 
                 "fatheduc = 0", 
                 vcov = vcovHC, type = "HC1")
```

Since this value is smaller than  
0.05  we reject the hypothesis that both instruments are exogenous at the level of  
5%. 