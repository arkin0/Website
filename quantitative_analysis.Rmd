---
title: "Quantitative Analysis"
output:
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```


Pour chaque algorithm, dire d'abord si il est paramétrique ou pas. 
Si il est flexible ou interprêtable (voir figure 2.7 https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)


# Linear Regression
## Simple Linear Regression 

Data set from Stock & Watson (2007), originally collected by T. Bergstrom, on subscriptions to 180 economics journals at US libraries, for the year 2000.
Bergstrom (2001) argues that commercial publishers are charging excessive prices for academic journals and also suggests ways that economists can deal with this problem. See http://www.econ.ucsb.edu/~tedb/Journals/jpricing.html

### Description 
```{r,  echo=T}
data("Journals", package = "AER")
journals <- Journals[, c("subs", "price")]
journals$citeprice <- Journals$price/Journals$citations
summary(journals)
```

### Estimation
The author wants to estimate the effect of the price per citation on the number of
library subscriptions.

```{r,  echo=T}
plot(log(subs) ~ log(citeprice), data = journals)
jour_lm <- lm(log(subs) ~ log(citeprice), data = journals)
abline(jour_lm)
```

Once the model is fitted, we can inspect the value of $log(citeprice)$.
```{r,  echo=T}
summary(jour_lm)
```


### Prediction
And finally prediction 

```{r,  echo=T}
lciteprice <- seq(from = -6, to = 4, by = 0.25)
jour_pred <- predict(jour_lm, interval = "prediction",
  newdata = data.frame(citeprice = exp(lciteprice)))  
plot(log(subs) ~ log(citeprice), data = journals)
lines(jour_pred[, 1] ~ lciteprice, col = 1)    
lines(jour_pred[, 2] ~ lciteprice, col = 1, lty = 2)
lines(jour_pred[, 3] ~ lciteprice, col = 1, lty = 2)
```


## Multiple Linear Regression 

Bierens and Ginther (Empirical Economics 2001) analyze determinants of wages. The US Census Bureau collected 28155 observations. We have a cross-section data on males aged 18 to 70 with positive annual income greater than US$ 50 in 1992 who are not self-employed or working without pay. Finally wages are deflated by the deflator of personal consumption expenditures for 1992.

### Description 
```{r,  echo=T}
data("CPS1988", package = "AER")
summary(CPS1988)
```

### Estimation 
```{r,  echo=T}
cps_lm <- lm(log(wage) ~ experience + I(experience^2) + education + ethnicity, data = CPS1988)
summary(cps_lm)
```

### Comparaison of two models : significativity of one variable with the Wald Test
Is there a difference in the average log-wage (controlling for experience and education) between Caucasian and African-American men? We want to test the relevance of the variable ethnicity is the regression. 

The Wald test works by testing the null hypothesis that a set of parameters is equal to some value. In the model being tested here, the null hypothesis is that the two coefficients of interest are simultaneously equal to zero. If the test fails to reject the null hypothesis, this suggests that removing the variables from the model will not substantially harm the fit of that model, since a predictor with a coefficient that is very small relative to its standard error is generally not doing much to help predict the dependent variable. 

```{r,  echo=T}
library(lmtest)
waldtest(cps_lm, . ~ . - ethnicity)
```

The p-value is less than the generally used criterion of 0.05 , so we are able to reject the null hypothesis, indicating that the coefficients are not equal. Because including statistically significant predictors should lead to better prediction (i.e., better model fit) we can conclude that including ethnicity results in a statistically significant improvement in the fit of the model.

# Linear Regression with Time series Data

## Infrastructure and "Naive" Methods

Standard time series class in R is "ts" and aimed at regulat series such as annual, quaterly, monthly. 

We take the example of the quaterly consumption of non-durables in the United Kingdom (Franses 1998)

```{r echo=TRUE, message=FALSE}
library(AER)
data("UKNonDurables")
```

Dygraphs provides rich facilities for charting time-series data in R and includes support for many interactive features including series/point highlighting, zooming, and panning.

```{r, echo=TRUE}
library(dygraphs)
dygraph(UKNonDurables, main = "quaterly consumption of non-durables in the UK") %>% 
  dyRangeSelector(dateWindow = c("1955-01-01", "1988-12-01"))
```


If you deal with irregular series, you might need to use other package such as the zoo
Generalization of “ts”: time stamps of arbitrary type. Numeric vectors or matrices, "index" attribute contains vector of time stamps (not just "tsp" attribute!). Regular series can be coerced back and forth between “ts” and “zoo” via as.zoo() and as.ts().
“zoo” more convenient for daily data (e.g., “Date” time stamps) or intraday data (e.g., “POSIXct” or “chron” time stamps). More details: Zeileis and Grothendieck (JSS 2005).

### Linear filtering

Finite moving averages is the most important one. You can implemente it with the filter() function in R. Applied to the UKDriverDeaths database (Harvey and Durbin, JRSS A 1986), we obtain the following.

```{r, echo=TRUE}
data("UKDriverDeaths")
plot(UKDriverDeaths)
lines(filter(UKDriverDeaths, c(1/2, rep(1, 11), 1/2)/12),
  col = 2)
```

### Decomposition into seasonal, trend and irregular components

In R, decompose() takes simple symmetric filter for extracting trend,derives seasonal component by averaging trend-adjusted observations from corresponding periods. stl() iteratively finds seasonal and trend components by loess smoothing in moving data windows. 

```{r, echo=TRUE}
dd_dec <- decompose(log(UKDriverDeaths))
dd_stl <- stl(log(UKDriverDeaths), s.window = 13)

```

### Stationarity, unit roots, and cointegration test
Many time series in macroeconomics and finance are nonstationary.With use the example from Franses 1998 which is a bivariate time series of average monthly European spot prices for black and white pepper (in US dollars per ton).

```{r, echo=TRUE}
data("PepperPrice")
plot(PepperPrice, plot.type = "single", col = 1:2)
legend("topleft", c("black", "white"), bty = "n",
 col = 1:2, lty = rep(1,2))
```

### Unit-root test

The augmented Dickey-Fuller (ADF) test can be done as follow : 
```{r, echo=TRUE}
library(tseries)
adf.test(log(PepperPrice[,"white"]))
```

### stationarity test
Kwiatkowski, Phillips, Schmidt and Shin (J. Econometrics 1992) create a test with two variants, one with a level stationnarity and the other one with a tredn stationnarity. 

```{r, echo=TRUE}
library(tseries)
kpss.test(log(PepperPrice[,"white"]))
```

### cointegration test

Engle-Granger two-step method Available in po.test() from tseries (named after Phillips and
Ouliaris, Econometrica 1990).

```{r, echo=TRUE}
po.test(log(PepperPrice))
```
if the p-value is lower thant 5%, it suggests that both series are cointegrated. 

# Diagnostics	

We give code here to validate the linear regression models. We will cover ; i) regression diagnostics: Comparison of statistics for full data set and for data with single observations deleted ; ii) diagnostic tests: Test for heteroskedasticity, autocorrelation, and
misspecification of the functional form, etc ; iii) Robust covariances: Covariance estimators that are consistent for a wide class of disturbance structures.

We will also give hints for alternative methods of regression : regression techniques that are robust to outliers and unusual observations and model quantiles of the conditional distribution of a variable.

Finally we show how to implement Instrumental Variables (IV) in the case of endogenous regressors. 

## Regression diagnostics

Find points that are not fitted as well as they should be or have undue influence on the fitting of the model. We use the paper of Besley, Kuh adn Welsch (1980) based on deletion of observations. *PublicSchools* data provide per capita Expenditure on public schools and per capita Income by state for the 50 states of the USA plus Washington, DC., for 1979.

```{r,  echo=T}
data("PublicSchools", package = "sandwich")
summary(PublicSchools)
```

Scatterplot with fitted linear model and three highlighted observations.We omit incomplete observations (Wisconsin) and scale income to be in 10,000 USD. 

```{r,  echo=T}
ps <- na.omit(PublicSchools)
ps$Income <- ps$Income / 10000
plot(Expenditure ~ Income, data = ps, ylim = c(230, 830))
ps_lm <- lm(Expenditure ~ Income, data = ps)
abline(ps_lm)
id <- c(2, 24, 48)
text(ps[id, 2:1], rownames(ps)[id], pos = 1, xpd = TRUE)
```

If you are familiar with outlier caracterization, use the following command 

```{r, eval=FALSE}
plot(ps_lm,which=1:6)
```

Alaska stands out in all plots, it has ; which = 1) a large residual ; which = 2) Upper tail of empirical distribution of residuals ; which = 3 ) Casts doubt on the assumption of homogeneous variances ; which = 4 and 6) Corresponds to an extraordinarily large Cook’s distance ; which = 5 and 6) Has the highest leverage. Deleting this observation might be an option.

## Diagnostic Tests

We test here for heteroskedasticity in cross-section regressions or disturbance autocorrelation in time series regressions.

We can reconsider Journals data as an example for cross-section regressions.

```{r, eval=FALSE }
data("Journals", package = "AER")
journals <- Journals[, c("subs", "price")]
journals$citeprice <- Journals$price/Journals$citations
journals$age <- 2000 - Journals$foundingyear
jour_lm <- lm(log(subs) ~ log(citeprice), data = journals)
```

### Testing for heteroskedasticity

With the Breusch-Pagan test 
```{r, echo=TRUE}
bptest(jour_lm)
```

or with the White test
```{r, echo=TRUE}
bptest(jour_lm, ~ log(citeprice) + I(log(citeprice)^2),
  data = journals)
```


f the p-value is below 5%, we reject the null hypothesis of homoskedasticity and conclude that there is heteroskedasticity.

### Testing the functional form

The assumption is make that $(ε|X) = 0$ and is crucial for consistency of the
least-squares estimator. Misspecification of the functional form, for example by omitting relevant variables, is a source of violation. The following command test if the specification is ok : 
```{r, echo=TRUE}
resettest(jour_lm)
```
if the p-value is lower than 5%, we reject the nul hypothesis that the functional form of the model is well specified. Here this is not the case. 

### Testing for autocorrelation
Time series regressions are often affected by autocorrelation (or serial correlation), just as disturbances in cross-section models are typically heteroskedastic.

Let us consider the model for US consumption function. 

```{r, echo=TRUE}
library("dynlm")
data("USMacroG", package = "AER")
consump1 <- dynlm(consumption ~ dpi + L(dpi),  data = USMacroG)
```

A classical test for autocorrelation is the Durbin-Watson. Dwtest() implements an exact procedure for computing the p value (for Gaussian data) and also provides a normal approximation for sufficiently large samples (both depending on the regressor matrix
X).

```{r, echo=TRUE}
dwtest(consump1)
```
the null hypothesis of no autocorrealation is here for example rejected.  

## Robust Standard Errors and Tests

As seen previously, in the presence of autocorrelation and/or heteroskedasticity, the covariance structure need to be adjusted. More often than not, form of the autocorrelation or
heteroskedasticity is unknown. In R, vcovHC() computes all versions of covariance estimators from a fitted linear model. 


```{r, echo=TRUE}
vcovHC(jour_lm)
```





# Support Vector Regression (SVR)

We use the reading skills database from the party package

```{r echo=TRUE, message=FALSE}
library(party)
print(head(readingSkills))
```

## Fitting SVR to the dataset
```{r, echo=TRUE}
library(e1071)
regressor = svm(formula = score ~ shoeSize,
                data = readingSkills,
                type = 'eps-regression',
                kernel = 'radial')
```

## Predicting a new result

```{r, echo=TRUE}
y_pred = predict(regressor, data.frame(shoeSize = 26))
```

## Visualising the SVR results
```{r echo=TRUE, message=FALSE}
library(ggplot2)
x_grid = seq(min(readingSkills$shoeSize), max(readingSkills$shoeSize), 0.1)
ggplot() +
  geom_point(aes(x = readingSkills$shoeSize, y = readingSkills$score),
             colour = 'red') +
  geom_line(aes(x = x_grid, y = predict(regressor, newdata = data.frame(shoeSize = x_grid))),
            colour = 'blue') +
  ggtitle('Truth or Bluff (SVR)') +
  xlab('shoeSize') +
  ylab('score')
```

# Decision Tree regression

##  Ames Housing data 

To illustrate various regularization concepts we will use the Ames Housing data that has been included in the AmesHousing package.
```{r, echo=TRUE}
library(AmesHousing)
library(rsample)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
print(head(ames_train))
```


## Fitting Decision Tree Regression to the dataset

```{r, echo=TRUE}
library(rpart)
regressor = rpart(formula = Sale_Price ~ .,
                  data = ames_train,
                  method = "anova",
                  control = rpart.control(minsplit = 1,minbucket = 1))

```

## Plot of the Graph
We can visualize our model with rpart.plot. rpart.plot has many plotting options, which we’ll leave to the reader to explore. However, in the default print it will show the percentage of data that fall to that node and the average sales price for that branch.

```{r echo=TRUE, message=FALSE}
library(rattle)
library(rpart.plot)
library(RColorBrewer)
rpart.plot(regressor)

```

# Random Forest Regression

##  Ames Housing data 

To illustrate various regularization concepts we will use the Ames Housing data that has been included in the AmesHousing package.
```{r, echo=TRUE}
library(AmesHousing)
library(rsample)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7)
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```


##Train the model

```{r, echo=TRUE, message=FALSE}
library(randomForest)
regressor = randomForest(Sale_Price ~.,
                         data = ames_train,
                         ntree = 5)
print(regressor)
```


## Test of the model 
```{r, echo=TRUE}
pred_1 = predict(regressor,ames_test)
```

Variable Importance 
```{r, echo=TRUE}
varImpPlot(regressor)
```
